import Foundation
import Speech
import AVFoundation

class VoiceManager: NSObject, ObservableObject {
    @Published var isListening = false
    @Published var hasRecordingPermission = false
    @Published var currentTranscript = "" // Real-time transcript updates
    @Published var isSpeechDetected = false // True when user is actively speaking

    private var audioEngine = AVAudioEngine()
    private var speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?

    private var completionHandler: ((String) -> Void)?
    private var partialTranscriptHandler: ((String) -> Void)?
    private var speechDetectedHandler: (() -> Void)? // Called immediately when speech starts
    private var latestTranscript = ""
    private var hasProcessedResult = false // Prevents duplicate processing

    // Silence detection
    private var silenceTimer: Timer?
    private let silenceThreshold: TimeInterval = 2.5 // 2.5 seconds of silence
    private var autoStopEnabled = true // Can be toggled

    // Continuous listening mode
    @Published var isContinuousMode = false
    private var continuousModeEnabled = false
    private var lastTranscriptLength = 0

    override init() {
        super.init()
        // Don't request permissions automatically - will be requested from onboarding
        checkExistingPermissions()
    }

    private func checkExistingPermissions() {
        // Check if permissions were already granted without requesting
        let speechStatus = SFSpeechRecognizer.authorizationStatus()
        let micStatus = AVAudioSession.sharedInstance().recordPermission

        DispatchQueue.main.async {
            self.hasRecordingPermission = (speechStatus == .authorized && micStatus == .granted)
        }
    }

    func requestPermissions() {
        print("üé§ Requesting speech recognition permissions...")
        SFSpeechRecognizer.requestAuthorization { [weak self] status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    print("‚úÖ Speech recognition authorized")
                    self?.requestRecordingPermission()
                case .denied:
                    print("‚ùå Speech recognition denied")
                    self?.hasRecordingPermission = false
                case .restricted:
                    print("‚ùå Speech recognition restricted")
                    self?.hasRecordingPermission = false
                case .notDetermined:
                    print("‚ùå Speech recognition not determined")
                    self?.hasRecordingPermission = false
                @unknown default:
                    print("‚ùå Speech recognition unknown status")
                    self?.hasRecordingPermission = false
                }
            }
        }
    }

    private func requestRecordingPermission() {
        print("üé§ Requesting microphone permissions...")
        AVAudioSession.sharedInstance().requestRecordPermission { [weak self] granted in
            DispatchQueue.main.async {
                if granted {
                    print("‚úÖ Microphone permission granted")
                    self?.hasRecordingPermission = true
                } else {
                    print("‚ùå Microphone permission denied")
                    self?.hasRecordingPermission = false
                }
            }
        }
    }

    func startListening(continuous: Bool = false, onPartialTranscript: ((String) -> Void)? = nil, onSpeechDetected: (() -> Void)? = nil, completion: @escaping (String) -> Void) {
        print("üéôÔ∏è Starting listening process... (continuous: \(continuous))")
        print("üìã Checking permissions - hasRecordingPermission: \(hasRecordingPermission)")

        // Prevent starting if already listening
        guard !isListening else {
            print("‚ö†Ô∏è Already listening, stopping existing session first...")
            stopListening()
            // Schedule restart after a brief delay to allow cleanup
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) {
                self.startListening(continuous: continuous, onPartialTranscript: onPartialTranscript, onSpeechDetected: onSpeechDetected, completion: completion)
            }
            return
        }

        guard hasRecordingPermission else {
            print("‚ùå Recording permission not granted")
            return
        }

        guard let speechRecognizer = speechRecognizer else {
            print("‚ùå Speech recognizer is nil")
            return
        }

        guard speechRecognizer.isAvailable else {
            print("‚ùå Speech recognizer is not available")
            return
        }

        print("‚úÖ All permissions and requirements met")
        continuousModeEnabled = continuous
        isContinuousMode = continuous
        completionHandler = completion
        partialTranscriptHandler = onPartialTranscript
        speechDetectedHandler = onSpeechDetected
        hasProcessedResult = false // Reset for new session
        lastTranscriptLength = 0

        // Clear previous transcript
        DispatchQueue.main.async {
            self.currentTranscript = ""
            self.isSpeechDetected = false
        }

        // Cancel previous task
        print("üîÑ Canceling previous recognition task...")
        recognitionTask?.cancel()
        recognitionTask = nil

        // Configure audio session
        print("üîß Configuring audio session...")
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            print("‚úÖ Audio session configured successfully")
        } catch {
            print("‚ùå Audio session setup failed: \(error)")
            return
        }

        // Create recognition request
        print("üìù Creating recognition request...")
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            print("‚ùå Unable to create recognition request")
            return
        }
        recognitionRequest.shouldReportPartialResults = true
        print("‚úÖ Recognition request created")

        // Configure audio engine
        print("üéõÔ∏è Configuring audio engine...")
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        print("üìä Recording format: \(recordingFormat)")

        // Remove any existing tap before installing new one
        if inputNode.numberOfInputs > 0 {
            inputNode.removeTap(onBus: 0)
            print("üßπ Removed existing tap before installing new one")
        }

        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            recognitionRequest.append(buffer)
        }

        audioEngine.prepare()
        print("üéõÔ∏è Audio engine prepared")

        do {
            try audioEngine.start()
            print("‚úÖ Audio engine started successfully")
            DispatchQueue.main.async {
                self.isListening = true
            }
        } catch {
            print("‚ùå Audio engine failed to start: \(error)")
            return
        }

        // Start recognition
        print("üéØ Starting speech recognition task...")
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in

            if let result = result {
                let newTranscript = result.bestTranscription.formattedString
                print("üé§ Transcript received: \(newTranscript)")

                // Detect speech start (any new words)
                let currentLength = newTranscript.count
                if currentLength > (self?.lastTranscriptLength ?? 0) {
                    // New speech detected!
                    DispatchQueue.main.async {
                        if !(self?.isSpeechDetected ?? false) {
                            print("üó£Ô∏è SPEECH DETECTED - User started speaking!")
                            self?.isSpeechDetected = true
                            self?.speechDetectedHandler?()
                        }
                    }
                    self?.lastTranscriptLength = currentLength
                }

                // Only update if we have a non-empty transcript or no previous transcript
                if !newTranscript.isEmpty || (self?.latestTranscript.isEmpty ?? true) {
                    self?.latestTranscript = newTranscript

                    // Update published property for real-time display
                    DispatchQueue.main.async {
                        self?.currentTranscript = newTranscript
                        self?.partialTranscriptHandler?(newTranscript)
                    }

                    // Reset silence timer on new transcript
                    self?.resetSilenceTimer()
                }

                if result.isFinal {
                    print("‚úÖ Final transcript: \(self?.latestTranscript ?? "")")
                    self?.invalidateSilenceTimer()
                    DispatchQueue.main.async {
                        guard let self = self, !self.hasProcessedResult else { return }
                        let transcript = self.latestTranscript
                        if !transcript.isEmpty {
                            self.hasProcessedResult = true
                            self.completionHandler?(transcript)

                            // In continuous mode, restart listening after processing
                            if self.continuousModeEnabled {
                                print("üîÑ Continuous mode: Restarting listening after transcript")
                                self.restartListeningForContinuousMode()
                            } else {
                                self.stopListening()
                            }
                        }
                    }
                    return
                }
            }

            if let error = error {
                print("‚ùå Recognition error: \(error)")
                print("üìù Current stored transcript: '\(self?.latestTranscript ?? "")'")

                self?.invalidateSilenceTimer()

                // Don't process on cancellation errors - these happen during normal stop
                let nsError = error as NSError
                if nsError.domain == "kLSRErrorDomain" && nsError.code == 301 {
                    print("üîÑ Recognition canceled - using stored transcript if available")
                    DispatchQueue.main.async {
                        guard let self = self, !self.hasProcessedResult else {
                            self?.stopListening()
                            return
                        }
                        let transcript = self.latestTranscript
                        if !transcript.isEmpty {
                            print("‚úÖ Processing stored transcript: \(transcript)")
                            self.hasProcessedResult = true
                            self.completionHandler?(transcript)
                        }
                        self.stopListening()
                    }
                } else {
                    // Other errors - use delayed processing
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                        guard let self = self, !self.hasProcessedResult else {
                            self?.stopListening()
                            return
                        }
                        let transcript = self.latestTranscript
                        if !transcript.isEmpty {
                            print("üîÑ Using stored transcript after delay: \(transcript)")
                            self.hasProcessedResult = true
                            self.completionHandler?(transcript)
                        }
                        self.stopListening()
                    }
                }
            }
        }
        print("üöÄ Speech recognition task started")
    }

    private func resetSilenceTimer() {
        guard autoStopEnabled else { return }

        DispatchQueue.main.async { [weak self] in
            self?.silenceTimer?.invalidate()
            self?.silenceTimer = Timer.scheduledTimer(withTimeInterval: self?.silenceThreshold ?? 2.5, repeats: false) { [weak self] _ in
                print("‚è±Ô∏è Silence detected - auto-stopping")
                self?.handleSilenceDetected()
            }
        }
    }

    private func invalidateSilenceTimer() {
        DispatchQueue.main.async { [weak self] in
            self?.silenceTimer?.invalidate()
            self?.silenceTimer = nil
        }
    }

    private func handleSilenceDetected() {
        guard autoStopEnabled, isListening else { return }

        print("üîá Silence threshold reached - finalizing transcript")

        DispatchQueue.main.async { [weak self] in
            guard let self = self, !self.hasProcessedResult else {
                self?.stopListening()
                return
            }
            let transcript = self.latestTranscript
            if !transcript.isEmpty {
                print("‚úÖ Auto-stop with transcript: \(transcript)")
                self.hasProcessedResult = true
                self.completionHandler?(transcript)

                // In continuous mode, restart listening after processing
                if self.continuousModeEnabled {
                    print("üîÑ Continuous mode: Restarting listening after silence")
                    self.restartListeningForContinuousMode()
                    return
                }
            }
            self.stopListening()
        }
    }

    private func restartListeningForContinuousMode() {
        // Briefly stop and restart to process the current transcript
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
            guard let self = self, self.continuousModeEnabled else { return }

            print("üîÑ Restarting listening in continuous mode...")

            // Stop current session
            self.audioEngine.stop()
            self.audioEngine.inputNode.removeTap(onBus: 0)
            self.recognitionRequest?.endAudio()
            self.recognitionTask?.cancel()

            // Reset state for new session
            self.latestTranscript = ""
            self.hasProcessedResult = false
            self.lastTranscriptLength = 0

            DispatchQueue.main.async {
                self.currentTranscript = ""
                self.isSpeechDetected = false
            }

            // Restart listening with same handlers
            let savedCompletionHandler = self.completionHandler
            let savedPartialHandler = self.partialTranscriptHandler
            let savedSpeechDetectedHandler = self.speechDetectedHandler

            if let completion = savedCompletionHandler {
                self.startListening(
                    continuous: true,
                    onPartialTranscript: savedPartialHandler,
                    onSpeechDetected: savedSpeechDetectedHandler,
                    completion: completion
                )
            }
        }
    }

    func stopListening() {
        print("üõë Stopping listening...")

        guard isListening else {
            print("‚ö†Ô∏è Already stopped listening")
            return
        }

        invalidateSilenceTimer()

        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()

        isListening = false
        continuousModeEnabled = false // Stop continuous mode
        recognitionRequest = nil
        recognitionTask = nil

        // Clear published transcript immediately
        DispatchQueue.main.async {
            self.currentTranscript = ""
            self.isContinuousMode = false
            self.isSpeechDetected = false
        }

        // Don't clear latestTranscript immediately, let delayed processing use it
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
            self.latestTranscript = ""
            self.lastTranscriptLength = 0
        }

        print("‚úÖ Listening stopped successfully")
    }

    // Enable/disable continuous listening mode
    func setContinuousMode(_ enabled: Bool) {
        continuousModeEnabled = enabled
        DispatchQueue.main.async {
            self.isContinuousMode = enabled
        }
        print("üîÑ Continuous mode set to: \(enabled)")
    }
}