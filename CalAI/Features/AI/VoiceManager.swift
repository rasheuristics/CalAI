import Foundation
import Speech
import AVFoundation

class VoiceManager: NSObject, ObservableObject {
    @Published var isListening = false
    @Published var hasRecordingPermission = false
    @Published var currentTranscript = "" // Real-time transcript updates

    private var audioEngine = AVAudioEngine()
    private var speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?

    private var completionHandler: ((String) -> Void)?
    private var partialTranscriptHandler: ((String) -> Void)?
    private var latestTranscript = ""
    private var hasProcessedResult = false // Prevents duplicate processing

    // Silence detection
    private var silenceTimer: Timer?
    private let silenceThreshold: TimeInterval = 2.5 // 2.5 seconds of silence
    private var autoStopEnabled = true // Can be toggled

    override init() {
        super.init()
        requestPermissions()
    }

    private func requestPermissions() {
        print("üé§ Requesting speech recognition permissions...")
        SFSpeechRecognizer.requestAuthorization { [weak self] status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    print("‚úÖ Speech recognition authorized")
                    self?.requestRecordingPermission()
                case .denied:
                    print("‚ùå Speech recognition denied")
                    self?.hasRecordingPermission = false
                case .restricted:
                    print("‚ùå Speech recognition restricted")
                    self?.hasRecordingPermission = false
                case .notDetermined:
                    print("‚ùå Speech recognition not determined")
                    self?.hasRecordingPermission = false
                @unknown default:
                    print("‚ùå Speech recognition unknown status")
                    self?.hasRecordingPermission = false
                }
            }
        }
    }

    private func requestRecordingPermission() {
        print("üé§ Requesting microphone permissions...")
        AVAudioSession.sharedInstance().requestRecordPermission { [weak self] granted in
            DispatchQueue.main.async {
                if granted {
                    print("‚úÖ Microphone permission granted")
                    self?.hasRecordingPermission = true
                } else {
                    print("‚ùå Microphone permission denied")
                    self?.hasRecordingPermission = false
                }
            }
        }
    }

    func startListening(onPartialTranscript: ((String) -> Void)? = nil, completion: @escaping (String) -> Void) {
        print("üéôÔ∏è Starting listening process...")
        print("üìã Checking permissions - hasRecordingPermission: \(hasRecordingPermission)")

        guard hasRecordingPermission else {
            print("‚ùå Recording permission not granted")
            return
        }

        guard let speechRecognizer = speechRecognizer else {
            print("‚ùå Speech recognizer is nil")
            return
        }

        guard speechRecognizer.isAvailable else {
            print("‚ùå Speech recognizer is not available")
            return
        }

        print("‚úÖ All permissions and requirements met")
        completionHandler = completion
        partialTranscriptHandler = onPartialTranscript
        hasProcessedResult = false // Reset for new session

        // Clear previous transcript
        DispatchQueue.main.async {
            self.currentTranscript = ""
        }

        // Cancel previous task
        print("üîÑ Canceling previous recognition task...")
        recognitionTask?.cancel()
        recognitionTask = nil

        // Configure audio session
        print("üîß Configuring audio session...")
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            print("‚úÖ Audio session configured successfully")
        } catch {
            print("‚ùå Audio session setup failed: \(error)")
            return
        }

        // Create recognition request
        print("üìù Creating recognition request...")
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            print("‚ùå Unable to create recognition request")
            return
        }
        recognitionRequest.shouldReportPartialResults = true
        print("‚úÖ Recognition request created")

        // Configure audio engine
        print("üéõÔ∏è Configuring audio engine...")
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        print("üìä Recording format: \(recordingFormat)")

        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            recognitionRequest.append(buffer)
        }

        audioEngine.prepare()
        print("üéõÔ∏è Audio engine prepared")

        do {
            try audioEngine.start()
            print("‚úÖ Audio engine started successfully")
            DispatchQueue.main.async {
                self.isListening = true
            }
        } catch {
            print("‚ùå Audio engine failed to start: \(error)")
            return
        }

        // Start recognition
        print("üéØ Starting speech recognition task...")
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in

            if let result = result {
                let newTranscript = result.bestTranscription.formattedString
                print("üé§ Transcript received: \(newTranscript)")

                // Only update if we have a non-empty transcript or no previous transcript
                if !newTranscript.isEmpty || (self?.latestTranscript.isEmpty ?? true) {
                    self?.latestTranscript = newTranscript

                    // Update published property for real-time display
                    DispatchQueue.main.async {
                        self?.currentTranscript = newTranscript
                        self?.partialTranscriptHandler?(newTranscript)
                    }

                    // Reset silence timer on new transcript
                    self?.resetSilenceTimer()
                }

                if result.isFinal {
                    print("‚úÖ Final transcript: \(self?.latestTranscript ?? "")")
                    self?.invalidateSilenceTimer()
                    DispatchQueue.main.async {
                        guard let self = self, !self.hasProcessedResult else { return }
                        let transcript = self.latestTranscript
                        if !transcript.isEmpty {
                            self.hasProcessedResult = true
                            self.completionHandler?(transcript)
                            self.stopListening()
                        }
                    }
                    return
                }
            }

            if let error = error {
                print("‚ùå Recognition error: \(error)")
                print("üìù Current stored transcript: '\(self?.latestTranscript ?? "")'")

                self?.invalidateSilenceTimer()

                // Don't process on cancellation errors - these happen during normal stop
                let nsError = error as NSError
                if nsError.domain == "kLSRErrorDomain" && nsError.code == 301 {
                    print("üîÑ Recognition canceled - using stored transcript if available")
                    DispatchQueue.main.async {
                        guard let self = self, !self.hasProcessedResult else {
                            self?.stopListening()
                            return
                        }
                        let transcript = self.latestTranscript
                        if !transcript.isEmpty {
                            print("‚úÖ Processing stored transcript: \(transcript)")
                            self.hasProcessedResult = true
                            self.completionHandler?(transcript)
                        }
                        self.stopListening()
                    }
                } else {
                    // Other errors - use delayed processing
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                        guard let self = self, !self.hasProcessedResult else {
                            self?.stopListening()
                            return
                        }
                        let transcript = self.latestTranscript
                        if !transcript.isEmpty {
                            print("üîÑ Using stored transcript after delay: \(transcript)")
                            self.hasProcessedResult = true
                            self.completionHandler?(transcript)
                        }
                        self.stopListening()
                    }
                }
            }
        }
        print("üöÄ Speech recognition task started")
    }

    private func resetSilenceTimer() {
        guard autoStopEnabled else { return }

        DispatchQueue.main.async { [weak self] in
            self?.silenceTimer?.invalidate()
            self?.silenceTimer = Timer.scheduledTimer(withTimeInterval: self?.silenceThreshold ?? 2.5, repeats: false) { [weak self] _ in
                print("‚è±Ô∏è Silence detected - auto-stopping")
                self?.handleSilenceDetected()
            }
        }
    }

    private func invalidateSilenceTimer() {
        DispatchQueue.main.async { [weak self] in
            self?.silenceTimer?.invalidate()
            self?.silenceTimer = nil
        }
    }

    private func handleSilenceDetected() {
        guard autoStopEnabled, isListening else { return }

        print("üîá Silence threshold reached - finalizing transcript")

        DispatchQueue.main.async { [weak self] in
            guard let self = self, !self.hasProcessedResult else {
                self?.stopListening()
                return
            }
            let transcript = self.latestTranscript
            if !transcript.isEmpty {
                print("‚úÖ Auto-stop with transcript: \(transcript)")
                self.hasProcessedResult = true
                self.completionHandler?(transcript)
            }
            self.stopListening()
        }
    }

    func stopListening() {
        print("üõë Stopping listening...")

        guard isListening else {
            print("‚ö†Ô∏è Already stopped listening")
            return
        }

        invalidateSilenceTimer()

        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()

        isListening = false
        recognitionRequest = nil
        recognitionTask = nil

        // Clear published transcript immediately
        DispatchQueue.main.async {
            self.currentTranscript = ""
        }

        // Don't clear latestTranscript immediately, let delayed processing use it
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
            self.latestTranscript = ""
        }

        print("‚úÖ Listening stopped successfully")
    }
}