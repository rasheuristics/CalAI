import Foundation
import Speech
import AVFoundation

class VoiceManager: NSObject, ObservableObject {
    @Published var isListening = false
    @Published var hasRecordingPermission = false

    private var audioEngine = AVAudioEngine()
    private var speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?

    private var completionHandler: ((String) -> Void)?
    private var latestTranscript = ""

    override init() {
        super.init()
        requestPermissions()
    }

    private func requestPermissions() {
        print("ğŸ¤ Requesting speech recognition permissions...")
        SFSpeechRecognizer.requestAuthorization { [weak self] status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    print("âœ… Speech recognition authorized")
                    self?.requestRecordingPermission()
                case .denied:
                    print("âŒ Speech recognition denied")
                    self?.hasRecordingPermission = false
                case .restricted:
                    print("âŒ Speech recognition restricted")
                    self?.hasRecordingPermission = false
                case .notDetermined:
                    print("âŒ Speech recognition not determined")
                    self?.hasRecordingPermission = false
                @unknown default:
                    print("âŒ Speech recognition unknown status")
                    self?.hasRecordingPermission = false
                }
            }
        }
    }

    private func requestRecordingPermission() {
        print("ğŸ¤ Requesting microphone permissions...")
        AVAudioSession.sharedInstance().requestRecordPermission { [weak self] granted in
            DispatchQueue.main.async {
                if granted {
                    print("âœ… Microphone permission granted")
                    self?.hasRecordingPermission = true
                } else {
                    print("âŒ Microphone permission denied")
                    self?.hasRecordingPermission = false
                }
            }
        }
    }

    func startListening(completion: @escaping (String) -> Void) {
        print("ğŸ™ï¸ Starting listening process...")
        print("ğŸ“‹ Checking permissions - hasRecordingPermission: \(hasRecordingPermission)")

        guard hasRecordingPermission else {
            print("âŒ Recording permission not granted")
            return
        }

        guard let speechRecognizer = speechRecognizer else {
            print("âŒ Speech recognizer is nil")
            return
        }

        guard speechRecognizer.isAvailable else {
            print("âŒ Speech recognizer is not available")
            return
        }

        print("âœ… All permissions and requirements met")
        completionHandler = completion

        // Cancel previous task
        print("ğŸ”„ Canceling previous recognition task...")
        recognitionTask?.cancel()
        recognitionTask = nil

        // Configure audio session
        print("ğŸ”§ Configuring audio session...")
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            print("âœ… Audio session configured successfully")
        } catch {
            print("âŒ Audio session setup failed: \(error)")
            return
        }

        // Create recognition request
        print("ğŸ“ Creating recognition request...")
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            print("âŒ Unable to create recognition request")
            return
        }
        recognitionRequest.shouldReportPartialResults = true
        print("âœ… Recognition request created")

        // Configure audio engine
        print("ğŸ›ï¸ Configuring audio engine...")
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        print("ğŸ“Š Recording format: \(recordingFormat)")

        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            recognitionRequest.append(buffer)
        }

        audioEngine.prepare()
        print("ğŸ›ï¸ Audio engine prepared")

        do {
            try audioEngine.start()
            print("âœ… Audio engine started successfully")
            DispatchQueue.main.async {
                self.isListening = true
            }
        } catch {
            print("âŒ Audio engine failed to start: \(error)")
            return
        }

        // Start recognition
        print("ğŸ¯ Starting speech recognition task...")
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in

            if let result = result {
                self?.latestTranscript = result.bestTranscription.formattedString
                print("ğŸ¤ Transcript received: \(self?.latestTranscript ?? "")")

                if result.isFinal {
                    print("âœ… Final transcript: \(self?.latestTranscript ?? "")")
                    DispatchQueue.main.async {
                        if let transcript = self?.latestTranscript, !transcript.isEmpty {
                            self?.completionHandler?(transcript)
                        }
                        self?.stopListening()
                    }
                    return
                }
            }

            if let error = error {
                print("âŒ Recognition error: \(error)")
                print("ğŸ“ Current stored transcript: '\(self?.latestTranscript ?? "")'")
                // Use stored transcript after delay
                DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                    if let transcript = self?.latestTranscript, !transcript.isEmpty {
                        print("ğŸ”„ Using stored transcript after delay: \(transcript)")
                        self?.completionHandler?(transcript)
                    }
                    self?.stopListening()
                }
            }
        }
        print("ğŸš€ Speech recognition task started")
    }

    func stopListening() {
        print("ğŸ›‘ Stopping listening...")

        guard isListening else {
            print("âš ï¸ Already stopped listening")
            return
        }

        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()

        isListening = false
        recognitionRequest = nil
        recognitionTask = nil

        // Don't clear transcript immediately, let delayed processing use it
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
            self.latestTranscript = ""
        }

        print("âœ… Listening stopped successfully")
    }
}